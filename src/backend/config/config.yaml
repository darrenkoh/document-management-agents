# Document Classification Agent Configuration

# Source directories to monitor for files (supports single path or list of paths)
# You can specify multiple directories to monitor:
# source_paths:
#   - "./input"
#   - "./documents"
#   - "/path/to/other/folder"
source_paths:
  - "/Volumes/Documents/Private"
  

# File extensions to process (REQUIRED - only files with these extensions will be processed)
# Empty list means NO files will be processed - you must specify extensions
# Common document formats:
file_extensions:
  - ".pdf"
  - ".docx"
  - ".doc"
  - ".txt"
  - ".png"
  - ".jpg"
  - ".jpeg"
  - ".heic"
  - ".gif"
  - ".tiff"

# Database configuration
database:
  # Path to the SQLite database file (high-performance DocumentDB)
  path: "data/databases/documents.db"
  # Vector database configuration
  vector_store:
    # Type of vector store: 'chromadb' or 'faiss'
    type: "chromadb"
    # Directory to persist vector data
    persist_directory: "data/vector_store"
    # Collection name for ChromaDB
    collection_name: "documents"
    # Embedding dimension (should match your embedding model)
    dimension: 4096
    # Distance metric for ChromaDB: 'l2', 'cosine', 'ip' (inner product)
    # 'cosine' is recommended for semantic search with normalized embeddings
    distance_metric: "cosine"

# OpenAI-compatible LLM configuration for operations (classification, embedding, summarization)
# Works with Ollama (with OpenAI compatibility), vLLM, LiteLLM, and other OpenAI-compatible servers
llm:
  # OpenAI-compatible API endpoint (e.g., http://localhost:11434/v1 for Ollama with OpenAI compatibility)
  endpoint: "http://spark-7819.local:11434/v1"
  # Model name to use for classification
  model: "nvidia/Qwen3-30B-A3B-NVFP4"
  # Timeout in seconds for API calls
  timeout: 300
  # Maximum number of tokens to predict (higher for reasoning models)
  num_predict: 12000
  # Embedding endpoint (separate from main LLM endpoint if needed)
  embedding_endpoint: "http://spark-7819.local:8080/v1"
  # Embedding model for semantic search (default: text-embedding-3-small)
  embedding_model: "Qwen/Qwen3-Embedding-8B"
  # Summarizer model for document summaries (default: gpt-3.5-turbo)
  summarizer_model: "nvidia/Qwen3-30B-A3B-NVFP4"
  # Retry configuration for failed LLM API calls
  retry:
    # Maximum number of retry attempts for failed API calls
    max_retries: 3
    # Delay in seconds between retry attempts (exponential backoff)
    base_delay: 1.0

# OCR configuration for text extraction from images/PDFs
ocr:
  # OCR provider to use: 'ollama', 'chandra', or 'hunyuan'
  provider: "chandra"
  # Maximum number of PDF pages to process with OCR (default: 12)
  # This limits OCR processing to the first N pages of PDFs to reduce processing time
  max_pages: 12
  
  # Ollama OCR configuration
  ollama:
    # OCR model name (e.g., 'deepseek-ocr:3b')
    model: "deepseek-ocr:3b"
    # Timeout for OCR operations in seconds
    timeout: 300
    # Maximum number of tokens to predict (higher for reasoning models)
    # Increase this if you see "done_reason='length'" in logs
    num_predict: 12000
    # Retry configuration
    retry:
      max_retries: 3
      base_delay: 1.0
  
  # Chandra OCR configuration (vLLM-based)
  chandra:
    # vLLM API endpoint (default: http://localhost:11435)
    endpoint: "http://spark-7819.local:11435"
    # Model name in vLLM (default: chandra)
    model: "chandra"
    # Timeout for OCR operations in seconds
    timeout: 1800
    # Maximum output tokens
    max_tokens: 16384
    # Frequency penalty to reduce repetition in generated text (0.0 to 2.0)
    frequency_penalty: 0.0
    # Whether to detect and retry on repetitive OCR output
    detect_repeat_tokens: false
    # Retry configuration
    retry:
      max_retries: 3
      base_delay: 1.0
  
  # HunyuanOCR configuration (vLLM-based)
  hunyuan:
    # vLLM API endpoint (default: http://localhost:11435)
    endpoint: "http://spark-7819.local:11435"
    # Model name in vLLM (default: tencent/HunyuanOCR)
    model: "tencent/HunyuanOCR"
    # Timeout for OCR operations in seconds
    timeout: 1800
    # Maximum output tokens
    max_tokens: 16384
    # Retry configuration
    retry:
      max_retries: 3
      base_delay: 1.0

# Optional: Predefined classification categories
# If empty, the agent will auto-detect categories
categories:
  - "Finance"
  - "Shopping"
  - "Travel"
  - "Home"
  - "School"
  - "Other"

# Classification prompt template
# Use {filename} and {content} as placeholders
# If not specified, a default prompt will be used
prompt_template: |
  You are an expert document classifier. Your task is to analyze the provided filename and document content, then assign exactly ONE main category and (when useful) up to 3 short sub-categories.

  Available main categories (choose exactly one):
  - Finance
  - Shopping
  - Travel
  - Home
  - School
  - Other

  Category definitions (strictly follow these):
  • Finance → bank statements, credit card statements, investments, 401k/IRA, tax forms (W2, 1099, etc.), pay stubs, loan documents, car/home/life insurance policies, financial transactions, invoices you need to pay or have paid for business/personal finances
  • Shopping → purchase receipts, online order confirmations, invoices for goods/services you bought (non-financial, non-travel), packing slips, return labels
  • Travel → flight tickets, hotel bookings, car rental confirmations, itineraries, visas, travel insurance, event tickets (concerts, museums, etc. when part of a trip)
  • Home → mortgage statements, property tax, home insurance, utility bills (electricity, water, internet, etc.), home improvement receipts/contracts, lease agreements, HOA documents
  • School → report cards, transcripts, class schedules, tuition bills, diplomas, assignment feedback, school-related letters
  • Other → everything else (medical, legal that isn’t finance/home, personal letters, certificates, etc.)

  Filename: {filename}
  Content:
  {content}

  Instructions:
  1. Read the filename and the full content carefully.
  2. Choose exactly ONE main category that fits best.
  3. If the document fits perfectly into Finance, Shopping, Travel, Home, or School and no extra detail is needed, leave sub-categories empty.
  4. Only add sub-categories when they provide meaningful additional context (especially important when main category is "Other", or when the document is a specific type inside a broad main category).
  5. CRITICAL: Each sub-category must be EXACTLY 1-2 words maximum. No exceptions. Use concise, specific terms like "visa", "tax_return", "receipt", "utility_bill". Never use full sentences or long descriptions. DO NOT generate sub-categories like "which_falls_under_the_travel_main_category..." - these are invalid.
  6. Maximum 3 sub-categories.

  Output strictly in this format (nothing else):

  MAIN: <chosen main category>
  SUB: <sub1>, <sub2>, <sub3>   (or leave blank after SUB: if none)

  Examples:
  MAIN: Finance
  SUB: tax_return, 1099, 2024

  MAIN: Shopping
  SUB: receipt, electronics

  MAIN: Other
  SUB: medical, dental, insurance

  MAIN: Travel
  SUB: visa, passport

  MAIN: Home
  SUB: utility, electricity

# Watch mode settings
watch:
  # Polling interval in seconds (for watch mode)
  interval: 5
  # Recursively watch subdirectories
  recursive: true
  # Optional: exclude paths from being ingested in watch/scan modes.
  # Use this to prevent generated/derived files from triggering re-processing loops.
  # NOTE: paths are expanded to absolute paths at runtime.
  exclude_paths:
    - "data/segmented_receipts"

# Optional: receipt image segmentation (SAM3)
segmentation:
  # When enabled, image inputs (png/jpg/jpeg/...) can be segmented into multiple receipts.
  # The original combined image is NOT ingested; each segment PNG is ingested as its own document.
  enable: true

  # Directory to write segmented receipt PNGs to.
  # This is excluded from watch/scan by default above to prevent loops.
  output_dir: "data/segmented_receipts"

  # Device for segmentation: auto|mps|cpu
  # - auto: use MPS on Apple Silicon if available, otherwise CPU
  device: "auto"

  # Path to SAM3 checkpoint (file or directory, depending on SAM3 install).
  # Required when enable=true.
  checkpoint_path: "~/.cache/huggingface/hub/models--facebook--sam3/snapshots/3c879f39826c281e95690f02c7821c4de09afae7/sam3.pt"

  # Text prompt used to ground receipt regions (SAM3 is detector+segmenter conditioned on text).
  # Tweak if your receipts are missed (e.g. "paper receipt", "invoice", "document").
  text_prompt: "receipt"

  # Confidence threshold for returned masks/boxes. Higher = fewer results.
  confidence_threshold: 0.5

  # Limits / heuristics
  max_masks: 128
  max_segments: 10
  min_area_ratio: 0.02
  min_width_px: 200
  min_height_px: 200
  min_fill_ratio: 0.35
  iou_dedup_threshold: 0.85
  bbox_padding_px: 12

# Web app configuration
webapp:
  # Port to run the web application on
  port: 8081
  # Host to bind the web application to (0.0.0.0 for all interfaces)
  host: "0.0.0.0"
  # Enable debug mode (set to false for production)
  debug: true

# Semantic search configuration
semantic_search:
  # Default number of results to return
  top_k: 30
  # Minimum similarity threshold (0.0 to 1.0)
  # Lower values return more results, higher values return more precise results
  min_similarity_threshold: 0.1
  # Maximum number of results to retrieve before filtering (for better ranking)
  max_candidates: 30
  # Enable debug logging for similarity calculations
  debug_similarity: true
  # Enable RAG (Retrieval-Augmented Generation) analysis
  # When enabled, retrieved documents are analyzed by LLM for relevance
  enable_rag: true
  # Minimum relevance score threshold for RAG filtering (0.0 to 1.0)
  # Documents below this score may be filtered out
  rag_relevance_threshold: 0.3
  # Enable BM25 keyword-based search for hybrid search
  # When enabled, combines BM25 (keyword) and semantic (vector) search
  enable_bm25: true
  # Weight for BM25 scores in hybrid search (0.0 to 1.0)
  # Higher values give more weight to keyword matching
  bm25_weight: 0.3
  # Weight for semantic search scores in hybrid search (0.0 to 1.0)
  # Higher values give more weight to semantic similarity
  semantic_weight: 0.7

# RAG answer generation prompt template
# Use {query} and {documents} as placeholders
# If not specified, a default prompt will be used
rag_answer_prompt_template: |
  You are an expert assistant that answers questions based on provided documents. You must answer ONLY the specific question asked, using ONLY information from the documents provided.

  CRITICAL: Answer the EXACT question asked. Do not provide information about related but different topics. If the question asks about property tax, do NOT provide mortgage interest information. If the question asks about a specific year, only provide information for that year.

  USER'S QUESTION: {query}

  DOCUMENTS PROVIDED:
  {documents}

  INSTRUCTIONS:
  1. Read the USER'S QUESTION above very carefully. Identify the specific topic, location, and year (if mentioned).
  2. Search through the DOCUMENTS PROVIDED above to find information that DIRECTLY answers the USER'S QUESTION.
  3. IGNORE any information in the documents that is not directly related to the USER'S QUESTION, even if it seems similar.
  4. If the question asks about "property tax", look for property tax amounts, NOT mortgage interest, NOT other taxes.
  5. If the question asks about a specific year (e.g., "2014"), only provide information for that exact year.
  6. If the question asks about a specific location (e.g., "Dublin"), only provide information for that location.
  7. When you find the relevant information, cite the source using [Document N] format (e.g., [Document 1], [Document 2]).
  8. If the specific information requested is not available in the documents, clearly state: "Based on the provided documents, I cannot find [specific information requested]."
  9. Provide a clear, direct answer that addresses the USER'S QUESTION exactly.
  10. At the end of your answer, provide a JSON list of all documents you referenced, in this exact format:
     CITATIONS: [{{"document_number": 1, "reason": "brief reason for citation"}}, {{"document_number": 2, "reason": "brief reason for citation"}}]

  Now answer the USER'S QUESTION using ONLY relevant information from the DOCUMENTS PROVIDED above:

  ANSWER:

# Document chunking configuration for embeddings
chunking:
  # Maximum characters per chunk (optimized for qwen3-embedding:8b with 32K context)
  chunk_size: 12000
  # Characters to overlap between chunks
  chunk_overlap: 200
  # Whether to generate summary embeddings for documents
  enable_summary_embedding: true

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "data/agent.log"

