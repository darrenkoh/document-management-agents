# Document Classification Agent Configuration

# Source directories to monitor for files (supports single path or list of paths)
# You can specify multiple directories to monitor:
# source_paths:
#   - "./input"
#   - "./documents"
#   - "/path/to/other/folder"
source_paths:
#  - "/Volumes/1TB/Private"
#  - "/Volumes/Documents/Private"
  - "/Volumes/Documents/Private"
  

# File extensions to process (REQUIRED - only files with these extensions will be processed)
# Empty list means NO files will be processed - you must specify extensions
# Common document formats:
file_extensions:
  - ".pdf"
  - ".docx"
  - ".doc"
  - ".txt"
  - ".png"
  - ".jpg"
  - ".jpeg"
  - ".gif"
  - ".tiff"

# Database configuration
database:
  # Path to the SQLite database file (high-performance DocumentDB)
  path: "data/databases/documents.db"
  # Vector database configuration
  vector_store:
    # Type of vector store: 'chromadb' or 'faiss'
    type: "chromadb"
    # Directory to persist vector data
    persist_directory: "data/vector_store"
    # Collection name for ChromaDB
    collection_name: "documents"
    # Embedding dimension (should match your embedding model)
    dimension: 4096
    # Distance metric for ChromaDB: 'l2', 'cosine', 'ip' (inner product)
    # 'cosine' is recommended for semantic search with normalized embeddings
    distance_metric: "cosine"

# Ollama configuration
ollama:
  # Ollama API endpoint (default: http://localhost:11434)
  endpoint: "http://spark-7819.local:11434"
  # Model name to use for classification
  model: "deepseek-r1:8b"
  # Timeout in seconds for API calls
  timeout: 300
  # Maximum number of tokens to predict (higher for reasoning models)
  num_predict: 12000
  # Embedding model for semantic search (default: qwen3-embedding:8b)
  embedding_model: "qwen3-embedding:8b"
  # Summarizer model for document summaries (default: deepseek-r1:8b)
  summarizer_model: "deepseek-r1:8b"
  # OCR model for fallback text extraction from images/PDFs
  # Set to 'chandra' to use Chandra OCR via vLLM (port 11435), 'hunyuan' to use HunyuanOCR via vLLM (port 11434), or 'deepseek-ocr:3b' for Ollama (port 11434)
  ocr_model: "hunyuan"
  # Timeout for OCR operations (can be longer than regular models)
  ocr_timeout: 300
  # Maximum number of PDF pages to process with OCR (default: 12)
  # This limits OCR processing to the first N pages of PDFs to reduce processing time
  max_ocr_pages: 12
  # Retry configuration for failed LLM API calls
  retry:
    # Maximum number of retry attempts for failed API calls
    max_retries: 3
    # Delay in seconds between retry attempts (exponential backoff)
    base_delay: 1.0

# Chandra OCR configuration (vLLM-based)
chandra:
  # vLLM API endpoint for Chandra OCR (default: http://localhost:11435)
  endpoint: "http://spark-7819.local:11435"
  # Model name for Chandra in vLLM (default: chandra)
  model: "chandra"
  # Timeout for Chandra OCR operations in seconds
  timeout: 1800
  # Maximum output tokens for Chandra OCR
  max_tokens: 16384
  # Retry configuration for failed Chandra API calls
  retry:
    # Maximum number of retry attempts for failed API calls
    max_retries: 3
    # Delay in seconds between retry attempts (exponential backoff)
    base_delay: 1.0
  # Frequency penalty to reduce repetition in generated text (0.0 to 2.0)
  frequency_penalty: 0.0
  # Whether to detect and retry on repetitive OCR output
  detect_repeat_tokens: false

# HunyuanOCR configuration (vLLM-based)
hunyuan:
  # vLLM API endpoint for HunyuanOCR (default: http://localhost:11434)
  endpoint: "http://spark-7819.local:11435"
  # Model name for HunyuanOCR in vLLM (default: tencent/HunyuanOCR)
  model: "tencent/HunyuanOCR"
  # Timeout for HunyuanOCR operations in seconds
  timeout: 1800
  # Maximum output tokens for HunyuanOCR
  max_tokens: 16384
  # Retry configuration for failed HunyuanOCR API calls
  retry:
    # Maximum number of retry attempts for failed API calls
    max_retries: 3
    # Delay in seconds between retry attempts (exponential backoff)
    base_delay: 1.0

# Optional: Predefined classification categories
# If empty, the agent will auto-detect categories
categories:
  - "Finance"
  - "Shopping"
  - "Travel"
  - "Home"
  - "School"
  - "Other"

# Classification prompt template
# Use {filename} and {content} as placeholders
# If not specified, a default prompt will be used
prompt_template: |
  You are an expert document classifier. Your task is to analyze the provided filename and document content, then assign exactly ONE main category and (when useful) up to 3 short sub-categories.

  Available main categories (choose exactly one):
  - Finance
  - Shopping
  - Travel
  - Home
  - School
  - Other

  Category definitions (strictly follow these):
  • Finance → bank statements, credit card statements, investments, 401k/IRA, tax forms (W2, 1099, etc.), pay stubs, loan documents, car/home/life insurance policies, financial transactions, invoices you need to pay or have paid for business/personal finances
  • Shopping → purchase receipts, online order confirmations, invoices for goods/services you bought (non-financial, non-travel), packing slips, return labels
  • Travel → flight tickets, hotel bookings, car rental confirmations, itineraries, visas, travel insurance, event tickets (concerts, museums, etc. when part of a trip)
  • Home → mortgage statements, property tax, home insurance, utility bills (electricity, water, internet, etc.), home improvement receipts/contracts, lease agreements, HOA documents
  • School → report cards, transcripts, class schedules, tuition bills, diplomas, assignment feedback, school-related letters
  • Other → everything else (medical, legal that isn’t finance/home, personal letters, certificates, etc.)

  Filename: {filename}
  Content:
  {content}

  Instructions:
  1. Read the filename and the full content carefully.
  2. Choose exactly ONE main category that fits best.
  3. If the document fits perfectly into Finance, Shopping, Travel, Home, or School and no extra detail is needed, leave sub-categories empty.
  4. Only add sub-categories when they provide meaningful additional context (especially important when main category is "Other", or when the document is a specific type inside a broad main category).
  5. CRITICAL: Each sub-category must be EXACTLY 1-2 words maximum. No exceptions. Use concise, specific terms like "visa", "tax_return", "receipt", "utility_bill". Never use full sentences or long descriptions. DO NOT generate sub-categories like "which_falls_under_the_travel_main_category..." - these are invalid.
  6. Maximum 3 sub-categories.

  Output strictly in this format (nothing else):

  MAIN: <chosen main category>
  SUB: <sub1>, <sub2>, <sub3>   (or leave blank after SUB: if none)

  Examples:
  MAIN: Finance
  SUB: tax_return, 1099, 2024

  MAIN: Shopping
  SUB: receipt, electronics

  MAIN: Other
  SUB: medical, dental, insurance

  MAIN: Travel
  SUB: visa, passport

  MAIN: Home
  SUB: utility, electricity

# Watch mode settings
watch:
  # Polling interval in seconds (for watch mode)
  interval: 5
  # Recursively watch subdirectories
  recursive: true

# Web app configuration
webapp:
  # Port to run the web application on
  port: 8081
  # Host to bind the web application to (0.0.0.0 for all interfaces)
  host: "0.0.0.0"
  # Enable debug mode (set to false for production)
  debug: true

# Semantic search configuration
semantic_search:
  # Default number of results to return
  top_k: 30
  # Minimum similarity threshold (0.0 to 1.0)
  # Lower values return more results, higher values return more precise results
  min_similarity_threshold: 0.1
  # Maximum number of results to retrieve before filtering (for better ranking)
  max_candidates: 30
  # Enable debug logging for similarity calculations
  debug_similarity: true
  # Enable RAG (Retrieval-Augmented Generation) analysis
  # When enabled, retrieved documents are analyzed by LLM for relevance
  enable_rag: true
  # Minimum relevance score threshold for RAG filtering (0.0 to 1.0)
  # Documents below this score may be filtered out
  rag_relevance_threshold: 0.3

# Document chunking configuration for embeddings
chunking:
  # Maximum characters per chunk (optimized for qwen3-embedding:8b with 32K context)
  chunk_size: 12000
  # Characters to overlap between chunks
  chunk_overlap: 200
  # Whether to generate summary embeddings for documents
  enable_summary_embedding: true

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "data/agent.log"

